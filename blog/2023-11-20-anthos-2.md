---
title: Hybrid Kubernetes in production pt. 2
description: >
    In this second installment of the Anthos series, we'll talk about how we run
    Anthos in Kartverket. 
slug: hybrid-kubernetes-in-production-part-2
authors:
  - name: Espen Henriksen
    title: Product Owner and Platform Developer
    url: https://espen.dev
    image_url: https://github.com/esphen.png
  - name: Bård Ove Hoel
    title: Tech Lead and Platform Developer
    url: https://github.com/bardove
    image_url: https://github.com/bardove.png
tags: [anthos, kubernetes, hybrid]
image: /img/skip.png
hide_table_of_contents: false
---

![Anthos in Google Cloud](img/anthos-4.jpg)

In this second installment of the Anthos series, we will talk about how we run
Anthos in [Kartverket](https://kartverket.no/en). We'll touch on the hardware,
the software, and the processes we use to keep it running.

By the end we hope that we'll have de-mystified Anthos a bit, and maybe given
you an idea of what it takes to run Anthos in production.

If you haven't read the first part, you can find it
[here](/blog/hybrid-kubernetes-in-production-part-1).

<!--truncate-->

This newsletter is the second of the three part series about Anthos in
Kartverket.

1. [Why we chose Anthos](/blog/hybrid-kubernetes-in-production-part-1)
2. How we run Anthos (You are here!)
3. Benefits and what we would have done differently (Coming soon)

## Installation and upgrades

We were some of the very early adopters of Anthos, so while doing the install we
did not have that many options for controlplane architecture. We wanted to use
existing underlying vmware infrastructure, so the nodes in our clusters are VMs,
provisioned by a script provided by Google. Our cluster is installed with
[kubeception](https://kubernetes.io/blog/2017/01/how-we-run-kubernetes-in-kubernetes-kubeception/) controlplane architechture, this
no longer the only, or recommended way. The recommended model is [Controlplane V2](https://cloud.google.com/anthos/clusters/docs/on-prem/latest/how-to/create-user-cluster-controlplane-v2),
here the controlplane nodes for the user cluster are in the user cluster itself. 

In the kubeception model, Kubernetes clusters are nested inside other Kubernetes clusters. Specifically, the control plane of the user clusters runs in the admin cluster on-premise. For each on-premise cluster created, a new set of nodes and a new control plane namespace are created on the admin cluster.

To install and make changes to the admin cluster, an admin workstation is required, which should be located in the same network as the admin cluster. All configurations are done using a CLI tool called “gkectl”. This tool handles most cluster administration tasks, and the configuration is provided with YAML files.

Our cluster setup is more or less static, and most cluster administration tasks involve upgrading or scaling existing clusters. The SKIP-Team has a cluster referred to as “sandbox”, which is always the first recipient of potentially breaking changes. After testing changes in both development and test environments, we roll out the changes to other clusters and do a production roll-out. This is mostly done outside work-hours, but we have not had downtime during cluster upgrades. Here is the general workflow for upgrading:

1. Upgrade your admin workstation to the target version of your upgrade.

2. From your admin workstation, upgrade your user clusters.

3. After all of the user clusters have been upgraded, you can upgrade your admin cluster from the admin workstation.

We have tried using [Terraform](https://www.terraform.io/) for as much of the configuration and configuration control as possible. This can not be done in the same way for clusters using the kubeception model. When we migrate to Controlplane V2 however, clusters can be managed via GCP, and we can finally start using terraform like we do for our GKE clusters, and GCP configuration in general.


Dataplane v2,
No Controlplane V2,
No Terraform

Updates, what are they like

## GCP integration

IAM / Groups / AIS

GUI screenshots

## Deployment

Deployment is a very interesting subject when it comes to Anthos. As a platform
team, it is our job to make sure that deployment is as quick and convenient as
possible for the product teams. This ambition has led us to iterate on our
processes, which has finally led us to a solution that both we and the
developers enjoy using.

### Iteration 1 - Terraform

When we first started out with Anthos, we had a very manual process for
deploying applications. A service account was provisioned in GCP, which allowed
the developers to impersonate a service account in Kubernetes, which in turn
allowed them to deploy apps using Terraform. This approach worked, but had a
decent amount of rough edges, and also would fail in ways that was hard to
debug.

With this approach the developers would have to manage their own Terraform
files, which most of the time was not within their area of expertise. And while
SKIP was able to build modules and tools to make this easier, it was still a
complex system that was hard to understand. Observability and discoverability
was also an issue.

Because of this we would consistently get feedback that this way of deploying
was too complicated and slow, in addition handling Terraform state was a pain.
As a platform team we're commited to our teams' well being, so we took this
seriously and looked at alternatives. It was around this time we adopted Anthos,
so thus Anthos Config Managment was a natural choice.

### Iteration 2 - Anthos Config Managment (ACM)

![Anthos Config Management architecture showing multiple Git repos deployed to a cluster](img/acm-1.png)

ACM is a set of tools that allows you to declaratively manage your Kubernetes
resources. Here we're mostly going to talk about Config Sync, which is a
[GitOps](https://about.gitlab.com/topics/gitops/) system for Kubernetes.

In a GitOps system, a team will have a Git repository that contains all the
Kubernetes resources that they want to deploy. This repository is then synced
to the Kubernetes cluster, and the resources are applied.

This can be likened to a pull-based system, where the GitOps tool (Config sync)
watches the repo for changes and pulls them into the cluster. This is in
contrast to a push-based system, where a script pushes the changes to a
cluster. It is therefore a dedicated system for deployment to Kubernetes, and
following the [UNIX philosophy](https://en.wikipedia.org/wiki/Unix_philosophy)
which focuses on doing that one thing well.

Using this kind of a workflow solves a lot of the issues around the terraform
based deployment that we had in the previous iteration. No longer do developers
need to set up a complicated integration with GCP service accounts and
impersonation, commiting a file to a Git repo will trigger a deployment. The
Git repo and the manifests in them also works as a state of truth for the
cluster, instead of having to reverse engineer what was deployed based on
terraform diffs and state.

![ACM UI showing a sync in progress](img/acm-2.gif)

It started well, however we soon ran into issues. The system would often take
a long time to reconcile the sync, and during the sync we would not have any
visibility into what was happening. This was not a deal breaker, but at the
same time this was not a particularily good developer experience.

We also ran into issues with implementing a level of self-service that we were
satisfied with. We wanted to give the developers the ability to provision their
own namespaces, but due to the multi-tenant nature of our clusters we also had
to make sure that teams were not able to write to each others' namespaces.
This was not a feature we were able to implement, but luckily our next iteration
had this built in, and we'll get back to that.

The final nail was the user interface. We simply expected more from a deployment
system than what ACM was able to provide. The only view into the deployment was
a long list of resources, which to a developer that is not an expert in
Kubernetes was not intuitive enough.

### Final iteration - Argo CD

![](img/argo-1.png)

This finally brought us to our current iteration. We had heard about Argo CD
before, but initially we were hesitant to add another system to our stack.
After ACM had introduced us to GitOps and we looked deeper into Argo CD, it was
obvious to us that Argo was more mature and would give our developers a better
user experience.

The killer feature here is the UI. Argo CD has an intuitive and user friendly
UI that gives the developers a good overview of what is deployed. Whenever
anything fails, it's immediately obvious which resource is failing, and Argo
allows you to drill down into the resource to see the details of the failure,
logs for deployments, Kubernetes events, etc.

![](img/argo-2.png)

The above photo illustrates this well. Here you can see a project with a number
of [Skiperator](https://github.com/kartverket/skiperator) applications. The
green checkmarks indicate that the application is synced and the green heart
indicates that the application is healthy. A developer can see the underlying
"owned" resources that Skiperator creates (such as a deployment, service, etc),
and get a look "behind the curtain" to see what is actually deployed. This helps
debugging and gives the developers a better insight into what is happening
during a deployment.

In terms of multi tenancy, Argo CD has a concept of projects. A project is a
set of namespaces that a team has access to, and a team can only use Argo to
sync to namespaces that are part of their project. The namespace allowlist can
also include wildcards, which sounds small but this solved our self-service
issue! With our apps-repo architecture, we would give a team a "prefix" (for
example `seeiendom-`), and that team would then be able to deploy to and create
any namespace that started with that prefix. If they tried to deploy to another
team's namespace they would be stopped, as they would not have access to that
prefix.

The prefix feature allows product teams to create a new directory in their apps
repo, which will then be synced to the cluster and deployed as a new namespace.
This is a very simple and intuitive workflow for creating short-lived
deployments, for example for pull requests, and it has been very well received
by the developers.

The apps-repo architecture will be a blog post itself at some point, so I won't
go too much into it.

And finally, if you're wondering what disaster recovery of an entire cluster
looks like with Argo CD, I leave you with the following video at the end.

<video controls width="100%" muted={true}>
  <source src="/img/argo-3.mov" type="video/mp4" />
</video>

## Hybrid Mesh

## Monitoring

### Google Cloud Monitoring

![Google Cloud Monitoring dashboard](img/gcp-monitoring-1.png)

GKE on-prem includes an agent that collects metrics from the cluster and sends
them to Google Cloud. This is a great feature which makes it relatively easy
to get started with metrics and monitoring. However, we have decided not to use
this feature, and instead use Grafana and LGTM for metrics and monitoring.

The agent is a great feature, and it's very easy to get started with. However,
we had some challenges:

The amount of metrics that are collected out of the box and sent to GCP
contributes a significant part of our total spend. It's not that we have a lot
of clusters, but the amount of metrics that are collected out of the box is very
high, and Anthos' setup didn't give us the control we needed to be able to
manage it in a good way. 

Note that this was before [Managed Service for
Prometheus](https://cloud.google.com/managed-prometheus?hl=en) was released with
more fine grained control over what metrics are collected. It is now the
recommended default, which should make it easier to manage.

Second, while Google Cloud Monitoring has a few nice dashboards ready for
Anthos, it feels inconsistent which dashboards work on-premise and which only
work in cloud as they are not labeled as such. This is not a big issue, but it's
a bit annoying. The bigger issue is that all the dashboards feel sluggish and
slow to load. Several of us have used Grafana before, so we're used to a
snappy and responsive UI. Google Cloud Monitoring feels slow and clunky in
comparison.

So the cost and the user experience were the main reasons we decided to look at
alternatives to Google Cloud Monitoring. We endewd up using Grafana and LGTM,
which we'll talk about next.

### Grafana with the LGTM stack

Used to use Stackdriver, now using LGTM

## Summary

To summarize: We like Anthos, and we think it's a great platform for running
hybrid Kubernetes. As a platform team we look at each feature on a case-by-case
basis, with the goal of giving our developers the best possible experience
instead of naïvely trying to use as much as possible of the platform. Because of
this we've decided to use Anthos for Kubernetes and service mesh, but not for
config sync and monitoring. This has given us a great platform that we're
confident will serve us well for years to come.

Stay tuned for the third and final part of this series, where we'll talk about
the benefits we've seen from Anthos, and what we would have done differently if
we were to start over.

_Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not
endorsed by or affiliated with Google in any way._